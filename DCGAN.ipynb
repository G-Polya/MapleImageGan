{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pickle\n",
    "import imageio\n",
    "import random\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 96ED-75D7\n",
      "\n",
      " C:\\Users\\wlska\\Documents\\MapleImageGan\\dataset 디렉터리\n",
      "\n",
      "2020-09-08  오후 10:56    <DIR>          .\n",
      "2020-09-08  오후 10:56    <DIR>          ..\n",
      "2020-09-08  오후 10:56    <DIR>          original_image\n",
      "2020-09-08  오후 10:56    <DIR>          새 폴더\n",
      "               0개 파일                   0 바이트\n",
      "               4개 디렉터리  310,445,670,400 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "%ls dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f9676d2a00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVXUlEQVR4nO3de5BU9ZUH8O+Znp4ZZhjkMbyEURSRhw+wVKLr7oaoJKibBTel0TKKtW5pVGLcMtlQ2a0S3TXlVnzspjQYrbCCRUQrJoqPoITVJLq+gCAiqICiDIwMD2FeMK8++8dctljOaWjmdvf09O/7qZrqmTO/2/d3Z+b07Tn3d38/UVUQUfEr6e0OEFF+MNmJAsFkJwoEk50oEEx2okAw2YkCESvZRWSGiHwkIptEZG62OkVE2Sc9vc4uIgkAHwOYDqAOwLsArlbV9em2qRmc0DG1yR7tj4iObsvWDuza0yXe90pjPO9UAJtU9RMAEJElAGYCSJvsY2qTeOfl2hi7JKIjmfqNrWm/F+dt/CgAhz5zXRQjogIUJ9m9twrmfwIRuVFEVorIyp27u2LsjojiiJPsdQAOfU8+GsD2wxup6qOqeo6qnjN0SCLG7ogojjjJ/i6AcSJykoiUAbgKwNLsdIuIsq3HBTpV7RSROQBeBpAAsEBVP8haz4goq+JU46GqLwF4KUt9IaIc4gg6okAw2YkCwWQnCgSTnSgQTHaiQDDZiQLBZCcKBJOdKBBMdqJAMNmJAsFkJwoEk50oEEx2okAw2YkCEesWV8qeFfvtLD5b2ofmZF83HPeFG9/c0Wxi3/63H5rYsKfiT1uwZ8kwE7vl5NfcttcN2BV7f8QzO1EwmOxEgWCyEwUi1v/sIrIFQBOALgCdqnpONjpFRNmXjQLd11SVFZQM/XTPWDe+7LZpJlZe3+g/iThT9nd0+k2bWkzsJz+9xG1bVX3AxEZ80Gpi+74x0e/XMThunn3eJfumuW2f+HmbiS2f+HzsPoSGb+OJAhE32RXAKyKySkRuzEaHiCg34r6Nv0BVt4vIMADLReRDVf3joQ2iF4EbAeCEUbysT9RbYp3ZVXV79NgA4LfoXtn18DZc/omoAPQ42UWkSkSqD34O4OsA1mWrY0SUXXHeVw8H8FvprgyXAviVqi7LSq+KxDttHSb27N3T3bYdJ9gKe+OYGrdtWVPKxJKtNgYAlR/b2Lh/WO+21cmn2pjzFzJgwz53+8aJx7lxT+voShMrr0q6bWWeWRwYM+66zG27bMKLGfchNHHWevsEwOQs9oWIcoiX3ogCwWQnCgSTnSgQvPB9BG1qC2zbO+3QzRuvmeM/gTOsdc+F/utr9We2COUV4gCgxBkZW9ra5Xeh08ZLBvqFtK533jex5Im1tt22en9fE842MXVG9gJAyrkKqyV+Y03aeHJOldt2xkZz9Repc+3w3qeemu9uPyhhC4fFgmd2okAw2YkCwWQnCgSTnSgQTHaiQLAafwSnPfk9Ezv10Z0m1nCZX8FtH2BjFXZzAECizVbj00m02Sp9+Wd73LaatL/i1PH+rLUlzXaiC2/7RO0od/sB6780sd3nDPH31WGPt22Q/+dY2mKP98Dx1W5bGXGGiSXa7BWJi//1Dnf71GX2GP587hK3bV/DMztRIJjsRIFgshMFgslOFAgW6I5g/MN2WOiWq2xxKuXfho1SW+9CxZf+EFh1ho92lPuvxf237rd92Lnbf95TTjCxzuPK3bblA5yilzNrrbbYmWEBILXdLitVNn6Q27bxBHvA5Xv9IqU35Lak0//ZSCqzQufw1xr8bzzXZEJTvn2L23TN3J9ntK9CwTM7USCY7ESBYLITBYLJThSIoxboRGQBgL8B0KCqp0exwQCeAjAGwBYAV6qqHXrUx7WOt2uIt55s73Gv/MSv0FXX2WLc/iGZv756o8wAoKS13cRSE8e4bRMb60xMzzrZbauD7X3u0myLcam9/oSTJWPsve/9N6eZnPKEwSa2f1iam9932p9Z0hlV180+hx5n/8zL7Y8FgD+6cOh7tiAKAGfPu9nEVs3z75MvBJn85T0OYMZhsbkAVqjqOAAroq+JqIAdNdmjFV4OH3g9E8DC6POFAGZluV9ElGU9/Z99uKrWA0D0aN/vRkTkRhFZKSIrd+72p04iotzLeYGOyz8RFYaeJvsOERkJANFjmuFIRFQoejpcdimA2QDujR6fy1qPesGME+2MpACwbZGtepd+Zu9dr9zhV827nFlR29OskOQNlx28Ps3ssnvskM6OccPdtjLWDu8t/8JuDwAdg+2xJXfsss952jh3e+2yPwfZ5p8HyvfaYbTNo/1qfHOtjR+32W/rXcHoqLJtW07177Ovemuz3f7MMW7bZKvd1yut/pWZr1faqzj5dtQzu4g8CeBNAONFpE5EbkB3kk8XkY0ApkdfE1EBO+qZXVWvTvOti7LcFyLKIY6gIwoEk50oELyf/Qhmjl9rYi9sON/E2v25D9HprFDUWuus3QSgdJ+t0A1Y689OmdpjRyZLyh/q0DakwsT6ffy52zbZ5TzHQDtr5v5R/d3tq/681cQ6d/sTYZY7S1vtrPULndJmz0kVu/wCXb9d9jkaT7TbO3OBAgCqEplfHh60rtHEvvvGtW7bT6YvyPh5c4VndqJAMNmJAsFkJwoEk50oEEx2okAEV40//Wd2ptBL3n7Tbbun3ZbT22rsnXvVn/mvmc3nHjCxC07+1G27cvkkE0t96lfNO7462cSSjXZoLwBoia1aHzjvVLdtv7W2mt5ylp2dtqTTr5q3jx1h+1XtXJIAUP37DfZ5/8r+DADg3Kkfm9jKTv8YyvfZ4+2otv3dfYZfzZcuO7HHwKXvu227Jp9iYuPv92feffYv7BWMWVXNbttc4ZmdKBBMdqJAMNmJAsFkJwpE0RboTl7+9258xKd2mOZPhq902z6wZ4KJ/c/uM02szU6UCgAoTdpi3oCkLdoBwLDVdhhtyzfPdtv+6eFfmNjHHc5aUwBu23yliW1dfqLbtvS0sSbWOtIWt964+j53+2EJW4xb3OTfN/7oHd8ysSHv+UWz7ZPsJAAVJ/n35LfW2bapcmdegJo2d/u2T+3w4lSrX3TrqrDpk9zqL8PV2GWfF2CBjohygMlOFAgmO1EgmOxEgchkDroFItIgIusOic0TkW0isib6uDS33SSiuDKpxj8O4CEAiw6LP6iqflm2gNVPt1Xvf2nwq96/fvU8E6v51FanW0f4VeT2pjIT+92aM9y2Y/fZfj2x6GduW8AOvTw16Q9LXTbhRRu0FxmOkb8vzzXVfnV657+/YmKvfNVeDQCAjy6yQ3a/Ockfwrq0yV4tkYT9nQ0bYieeAIAdk+zvzK7+ll5qsD+Tyd3PX2Fi112T33Xherr8ExH1MXH+Z58jImujt/l2EnAiKig9Tfb5AMYCmAKgHsD96RpyrTeiwtCjZFfVHarapaopAI8B8JdUAdd6IyoUPRouKyIjD67iCuByAOuO1L6gdNrXt2d+b2eMBYD+223hrf82O8yyeVS5u31Jk/3xVtb7r6+fXG+HdNYk+rlti8Htg7aY2C9u9S/qDPqDLbBVnu7fv1+StD/Higq79NJZNdvc7Vem4l2Nbh9il9ACgHF3vmeD18Ta1TE7arJHyz9NA1AjInUA7gQwTUSmAFAAWwDclMM+ElEW9HT5p1/moC9ElEMcQUcUCCY7USCY7ESBKIrJKxY11pjYqGeTbtu6S2y1tsQWawEA/evsuABJ+TOrekpbbTV/0Mf+Wm9DLrbDSpMS1qXK/7z2MTd+32xbtl61xw6hBYCuNvsza9lrh8Curhjtbl9TaScBKZbRITyzEwWCyU4UCCY7USCY7ESBKIoCnSddIa2k2RZwypwlgwCgrMkW81IJ2zbpT+wKiG3bNNr/kf/3xF85UX/oZbH6Wj9/5t3v3eb8Ht6t9Z9ksC2AVm61P/Oulf5d6hudVaVO6bfX39cxkHJ/SHU+8cxOFAgmO1EgmOxEgWCyEwWCyU4UiKKtxpe2+IMcS9ptNb7fTr9yX7bXTlQhbd4QWr/SWrrfxjrTzEcxKBFW5d2Tbnjw4AH2ckfHJn+G270T7XNUbbe/39I2/3fevy435z8ZYGcEzjee2YkCwWQnCgSTnSgQmSz/VCsir4rIBhH5QES+H8UHi8hyEdkYPXLueKIClkmBrhPAHaq6WkSqAawSkeUArgewQlXvFZG5AOYC+FHuunpsSlv8+8bL99piWlmTX8xLNNrhm40T7Wta4oBf7Bn5ql1IZ9GyBW7bY1lSKTQrznjSxE77bI7btrzBFuiSrXa4bTqlLfZ3mdrvVFqPUWpX7y+qlMnyT/Wqujr6vAnABgCjAMwEsDBqthDArFx1kojiO6b/2UVkDICzALwNYPjBueOjx2HZ7hwRZU/GyS4i/QE8A+B2VfWXwPS34/JPRAUgo2QXkSS6E32xqv4mCu8QkZHR90cCaPC25fJPRIUhkxVhBN2LQmxQ1QcO+dZSALMB3Bs9PpeTHmZg8Ta7jnrjWH+oWmV95hNGypf2DYyW2AJdiV8LBDrsN2oSLMQdq8oSO2Fk2S7/xJHY789NkKmy5syLeZ2Vtg+tQ/1+3fPoOz3uU7ZkUo2/AMC1AN4XkTVR7MfoTvKnReQGAJ8DsKvNE1HByGT5p9cBpHu5vCi73SGiXOEIOqJAMNmJAsFkJwpEUdzPnrilwsSavuW/jlVts9X4fjv99Z823XKSiR3/um1bvanZ3X7jvGo3TvGlmxG4anvm1XRPqTf0Wfy/pYp/3G5if5j4Qqz95xLP7ESBYLITBYLJThQIJjtRIIqiQIfdX5pQ2T7/JrzKXXYIa/Kt9W7bs+6xk0DufXakiZVssYUaANg47VU3TvE9cvNDbvwH/3xLrOctabcFPkn4Q2CXFXAxzsMzO1EgmOxEgWCyEwWCyU4UCCY7USCKoxrvkJQ/SUXigK226mlj3bYp/cIJxuoWZckFFf55atCqnSa2b3KNiUnmc5gUDZ7ZiQLBZCcKBJOdKBBxln+aJyLbRGRN9HFp7rtLRD0VZ/knAHhQVe/LXfcy1GWrZt7a6ABQ/rkdWvvrV5e4ba/fckmsblH+fXTLUBMb8YatxqVbsquko3grd5lMOFkP4ODKL00icnD5JyLqQ+Is/wQAc0RkrYgs4CquRIUtzvJP8wGMBTAF3Wf++9Nsx+WfiApAj5d/UtUdqtqlqikAjwGY6m3L5Z+ICkMm1Xh3+aeD67xFLgewLvvdI6JsibP809UiMgWAAtgC4Kac9DATXfbfg2SLP65V9rfF2lXJgXYT27HIVoCpd2y+8hETu/Shy02sZUJ4v7M4yz+9lP3uEFGucAQdUSCY7ESBYLITBaJo72fPla4NG03smpMae6EnlLHde52gX6ATdYbWDimO8WI8sxMFgslOFAgmO1EgmOxEgWCyEwWiKKrx268/3cSGrWp12+6eVmtiCfEGCFLRUGf9ti5/koqyrXZykxdXv5z1LvUGntmJAsFkJwoEk50oEEx2okAURYHuh7c+ZWKLv2KLdgAw4icVJlYuSbftnONXmNg/fee7TstXjtxB6lUb7h9nYuMWdLhtpSXNtMRFgGd2okAw2YkCwWQnCkQmE05WiMg7IvJetPzTXVF8sIgsF5GN0WNx3AdIVKQyKdC1AbhQVZujKaVfF5HfAfg7ACtU9V4RmQtgLoAf5bCvaXWpHQGn7X4BBrAFunT+2mn65QSOtutrfnC+HQH3/CPT8t+RXnbUM7t2a46+TEYfCmAmgIVRfCGAWTnpIRFlRaaLRCSiaaQbACxX1bcBDI/WgTu4Htyw3HWTiOLKKNmjlV+mABgNYKqI+BexHVz+iagwHFM1XlX3AngNwAwAOw6uChM9NqTZhss/ERWATKrxQ0VkYPR5PwAXA/gQwFIAs6NmswE8l6tOElF8mVTjRwJYKCIJdL84PK2qL4jImwCeFpEbAHwO4Ioc9vOIrhuwy8Tmzf9bt+24TvcNSMY6xxwwsdFlu2M9J+VWW8oOh9Z333fbVr0+JNfd6TWZLP+0Ft1rsh8e3w3golx0ioiyjyPoiALBZCcKBJOdKBBFcT+7Z/aUt9z4+Ir6WM+7+cL/irU9FYbWy7/ixm8evjjPPckfntmJAsFkJwoEk50oEEx2okAw2YkCUbTV+DuHru/tLlCBKC+xE5l8cb5/nptV1ezGiwHP7ESBYLITBYLJThQIJjtRIIq2QEd00K0Dt9rYd+b3Qk96F8/sRIFgshMFgslOFIg4yz/NE5FtIrIm+rg0990lop6Ks/wTADyoqvflrntElC2ZTDipALzln4ioD4mz/BMAzBGRtSKygKu4EhW2OMs/zQcwFsAUAPUA7ve25fJPRIWhx8s/qeqO6EUgBeAxAFPTbMPln4gKQI+Xfzq4zlvkcgDrctNFIsqGOMs/PSEiU9BdrNsC4KbcdZOI4oqz/NO1OekREeUER9ARBYLJThQIJjtRIJjsRIFgshMFgslOFAgmO1EgmOxEgWCyEwWCyU4UCCY7USCY7ESBYLITBYLJThQIJjtRIJjsRIFgshMFgslOFAgmO1EgmOxEgZDu1Z3ytDORnQA+i76sAbArbzvPHx5X31NMx3aiqg71vpHXZP9/OxZZqarn9MrOc4jH1fcU87Edim/jiQLBZCcKRG8m+6O9uO9c4nH1PcV8bP+n1/5nJ6L84tt4okDkPdlFZIaIfCQim0Rkbr73n00iskBEGkRk3SGxwSKyXEQ2Ro+DerOPPSEitSLyqohsEJEPROT7UbxPH5uIVIjIOyLyXnRcd0XxPn1cmcprskcrwT4M4BIAkwBcLSKT8tmHLHscwIzDYnMBrFDVcQBWRF/3NZ0A7lDViQDOA3Br9Hvq68fWBuBCVZ0MYAqAGSJyHvr+cWUk32f2qQA2qeonqtoOYAmAmXnuQ9ao6h8B7DksPBPAwujzhQBm5bVTWaCq9aq6Ovq8CcAGAKPQx49NuzVHXyajD0UfP65M5TvZRwHYesjXdVGsmAxX1XqgO2kADOvl/sQiImPQvWT32yiCYxORhIisAdAAYLmqFsVxZSLfyS5OjJcDCpSI9AfwDIDbVbWxt/uTDarapapTAIwGMFVETu/tPuVLvpO9DkDtIV+PBrA9z33ItR0iMhIAoseGXu5Pj4hIEt2JvlhVfxOFi+LYAEBV9wJ4Dd01l6I5riPJd7K/C2CciJwkImUArgKwNM99yLWlAGZHn88G8Fwv9qVHREQA/BLABlV94JBv9eljE5GhIjIw+rwfgIsBfIg+flyZyvugGhG5FMB/AEgAWKCq9+S1A1kkIk8CmIbuu6Z2ALgTwLMAngZwAoDPAVyhqocX8QqaiPwlgD8BeB9AKgr/GN3/t/fZYxORM9FdgEug+0T3tKreLSJD0IePK1McQUcUCI6gIwoEk50oEEx2okAw2YkCwWQnCgSTnSgQTHaiQDDZiQLxv8wpDZ79YYw6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL.Image as pilimg\n",
    "import numpy as np\n",
    "\n",
    "image = pilimg.open('dataset/original_image/0.png')\n",
    "image_pix = np.array(image)\n",
    "plt.imshow(image_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.dterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 126\n",
       "    Root location: dataset/\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 방식을 지정한다.\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 데이터를 PyTorch의 Tensor 형식으로 바꾼다.\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5, 0.5, 0.5)) # 픽셀값 0 ~ 1 -> -1 ~ 1\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=\"dataset/\", transform=transform)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "leave_log = True\n",
    "if leave_log:\n",
    "    result_dir = 'GAN_generated_images/'\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GAN의 생성자(Generator)\n",
    "# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    # 네트워크 구조\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=100,out_channels=64*8,kernel_size=4,stride=1,padding=0, bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64*8,out_channels=64*4,kernel_size=5,stride=2,padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64*4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64*4,out_channels=64*2,kernel_size=5,stride=2,padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64*2,out_channels=64,kernel_size=5,stride=2,padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64,out_channels=3,kernel_size=5,stride=2,padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    # (batch_size x 100) 크기의 랜덤 벡터를 받아 \n",
    "    # 이미지를 (batch_size x 1 x 28 x 28) 크기로 출력한다.\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "#load weights to test the model\n",
    "#netG.load_state_dict(torch.load('weights/netG_epoch_24.pth'))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DCGAN의 구분자\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    # 네트워크 구조\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels=3,out_channels=64, kernel_size=4,stride=2,padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64,out_channels=64*2, kernel_size=4,stride=2,padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=64*2,out_channels=64*4, kernel_size=4,stride=2,padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=64*4,out_channels=64*8, kernel_size=4,stride=2,padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(in_channels=64*8,out_channels=1, kernel_size=4,stride=1,padding=0, bias=False),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    # (batch_size x 1 x 28 x 28) 크기의 이미지를 받아\n",
    "    # 이미지가 진짜일 확률을 0~1 사이로 출력한다.\n",
    "    def forward(self, inputs):\n",
    "        output = self.main(inputs)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "#load weights to test the model \n",
    "#netD.load_state_dict(torch.load('weights/netD_epoch_24.pth'))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(128, 100, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def square_plot(data, path):\n",
    "    \"\"\"Take an array of shape (n, height, width) or (n, height, width , 3)\n",
    "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
    "\n",
    "    if type(data) == list:\n",
    "\t    data = np.concatenate(data)\n",
    "    # normalize data for display\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "\n",
    "    padding = (((0, n ** 2 - data.shape[0]) ,\n",
    "                (0, 1), (0, 1))  # add some space between filters\n",
    "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
    "    data = np.pad(data , padding, mode='constant' , constant_values=1)  # pad with ones (white)\n",
    "\n",
    "    # tilethe filters into an image\n",
    "    data = data.reshape((n , n) + data.shape[1:]).transpose((0 , 2 , 1 , 3) + tuple(range(4 , data.ndim + 1)))\n",
    "\n",
    "    data = data.reshape((n * data.shape[1] , n * data.shape[3]) + data.shape[4:])\n",
    "\n",
    "    plt.imsave(path, data, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-ecd4dccd5415>:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  z_fixed = Variable(torch.randn(5 * 5, 300), volatile=True)\n"
     ]
    }
   ],
   "source": [
    "if leave_log:\n",
    "    train_hist = {}\n",
    "    train_hist['D_losses'] = []\n",
    "    train_hist['G_losses'] = []\n",
    "    generated_images = []\n",
    "    \n",
    "z_fixed = Variable(torch.randn(5 * 5, 300), volatile=True)\n",
    "if use_gpu:\n",
    "    z_fixed = z_fixed.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (2 x 2). Kernel size: (4 x 4). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-6fb8b64da0de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# 진짜 이미지를 구분자에 넣는다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mD_result_from_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m# 구분자의 출력값이 정답지인 1에서 멀수록 loss가 높아진다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mD_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_result_from_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-ebb021098046>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# 이미지가 진짜일 확률을 0~1 사이로 출력한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 415\u001b[1;33m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (2 x 2). Kernel size: (4 x 4). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "### 모델 학습을 위한 반복문\n",
    "# 데이터셋을 100번 돌며 학습한다.\n",
    "for epoch in range(100):\n",
    "    \n",
    "    if leave_log:\n",
    "        D_losses = []\n",
    "        G_losses = []\n",
    "    \n",
    "    # 한번에 batch_size만큼 데이터를 가져온다.\n",
    "    for real_data, _ in dataloader:\n",
    "        batch_size = real_data.size(0)\n",
    "        \n",
    "        # 데이터를 pytorch의 변수로 변환한다.\n",
    "        real_data = Variable(real_data).to(\"cuda\")\n",
    "\n",
    "        ### 구분자 학습시키기\n",
    "\n",
    "        # 이미지가 진짜일 때 정답 값은 1이고 가짜일 때는 0이다.\n",
    "        # 정답지에 해당하는 변수를 만든다.\n",
    "        target_real = Variable(torch.ones(batch_size, 1)).to(\"cuda\")\n",
    "        target_fake = Variable(torch.zeros(batch_size, 1)).to(\"cuda\")\n",
    "         \n",
    "        if use_gpu:\n",
    "            real_data, target_real, target_fake = real_data.cuda(), target_real.cuda(), target_fake.cuda()\n",
    "            \n",
    "        # 진짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_real = netD(real_data)\n",
    "        # 구분자의 출력값이 정답지인 1에서 멀수록 loss가 높아진다.\n",
    "        D_loss_real = criterion(D_result_from_real, target_real)\n",
    "\n",
    "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
    "        z = Variable(torch.randn((batch_size, 100)))\n",
    "        \n",
    "        if use_gpu:\n",
    "            z = z.cuda()\n",
    "            \n",
    "        # 생성자로 가짜 이미지를 생성한다.\n",
    "        fake_data = G(z)\n",
    "        \n",
    "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_fake = netD(fake_data)\n",
    "        # 구분자의 출력값이 정답지인 0에서 멀수록 loss가 높아진다.\n",
    "        D_loss_fake = criterion(D_result_from_fake, target_fake)\n",
    "        \n",
    "        # 구분자의 loss는 두 문제에서 계산된 loss의 합이다.\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        \n",
    "        # 구분자의 매개 변수의 미분값을 0으로 초기화한다.\n",
    "        D.zero_grad()\n",
    "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
    "        D_loss.backward()\n",
    "        # 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        if leave_log:\n",
    "            D_losses.append(D_loss.data)\n",
    "\n",
    "        # train generator G\n",
    "\n",
    "        ### 생성자 학습시키기\n",
    "        \n",
    "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
    "        z = Variable(torch.randn((batch_size, 100)))\n",
    "        \n",
    "        if use_gpu:\n",
    "            z = z.cuda()\n",
    "        \n",
    "        # 생성자로 가짜 이미지를 생성한다.\n",
    "        fake_data = G(z)\n",
    "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_fake = D(fake_data)\n",
    "        # 생성자의 입장에서 구분자의 출력값이 1에서 멀수록 loss가 높아진다.\n",
    "        G_loss = criterion(D_result_from_fake, target_real)\n",
    "        \n",
    "        # 생성자의 매개 변수의 미분값을 0으로 초기화한다.\n",
    "        G.zero_grad()\n",
    "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
    "        G_loss.backward()\n",
    "        # 최적화 기법을 이용해 생성자의 매개 변수를 업데이트한다.\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        if leave_log:\n",
    "            G_losses.append(G_loss.data)\n",
    "    if leave_log:\n",
    "        true_positive_rate = (D_result_from_real > 0.5).float().mean().data\n",
    "        true_negative_rate = (D_result_from_fake < 0.5).float().mean().data\n",
    "        base_message = (\"Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} \"\n",
    "                        \"True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}\"\n",
    "                       )\n",
    "        message = base_message.format(\n",
    "                    epoch=epoch,\n",
    "                    d_loss=sum(D_losses)/len(D_losses),\n",
    "                    g_loss=sum(G_losses)/len(G_losses),\n",
    "                    tpr=true_positive_rate,\n",
    "                    tnr=true_negative_rate\n",
    "        )\n",
    "        print(message)\n",
    "    \n",
    "    if leave_log:\n",
    "        fake_data_fixed = G(z_fixed)\n",
    "        image_path = result_dir + '/epoch{}.png'.format(epoch)\n",
    "        square_plot(fake_data_fixed.view(25, 28, 28).cpu().data.numpy(), path=image_path)\n",
    "        generated_images.append(image_path)\n",
    "    \n",
    "    if leave_log:\n",
    "        train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
    "        train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n",
    "\n",
    "torch.save(G.state_dict(), \"dcgan_generator.pkl\")\n",
    "torch.save(D.state_dict(), \"dcgan_discriminator.pkl\")\n",
    "with open('dcgan_train_history.pkl', 'wb') as f:\n",
    "    pickle.dump(train_hist, f)\n",
    "\n",
    "generated_image_array = [imageio.imread(generated_image) for generated_image in generated_images]\n",
    "imageio.mimsave(result_dir + '//DCGAN_generation.gif', generated_image_array, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Zero images were written.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-3aeea8b9b446>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgenerated_image_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimageio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimageio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/DCGAN_generation.gif'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerated_image_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mmimwrite\u001b[1;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;31m# be a generator. The damage is done, but we want to error when it happens.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwritten\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Zero images were written.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[1;31m# Return a result if there is any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Zero images were written."
     ]
    }
   ],
   "source": [
    "generated_image_array = [imageio.imread(generated_image) for generated_image in generated_images]\n",
    "\n",
    "imageio.mimsave(result_dir + '/DCGAN_generation.gif', generated_image_array, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
